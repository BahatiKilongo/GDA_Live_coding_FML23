{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GDA Implementation.\n",
        "\n",
        "Implement the Gaussian Discriminant Analysis (GDA) learning algorithm following the steps as discussed in class.\n",
        "\n",
        "INSTRUCTION: Rename your notebook as: <br>\n",
        "`firstName_LastName_Live_coding_GDA.ipynb`.\n",
        "\n",
        "Notes: \n",
        "* Do not use any built-in functions to complete a task;\n",
        "* Do not import additional libraries."
      ],
      "metadata": {
        "id": "g17Z46tmw2oZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aT5nlL-QTKwv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data\n",
        "def generate_data():\n",
        "  x, y = make_classification(n_samples= 1000, n_features=3, n_redundant=0, \n",
        "                           n_informative=3, random_state=1, \n",
        "                           n_clusters_per_class=1)\n",
        "  \n",
        "  return x,y\n",
        "\n",
        "x,y= generate_data() # get data\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-lL4Yq9Tbzn",
        "outputId": "d9427f06-a640-4f31-f2b8-79ff2d7474d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 3) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(x,y, train_size= 0.8):\n",
        "    # shuffle the data to randomize the train/test split\n",
        "    data = np.hstack((x,y.reshape(-1,1)))\n",
        "    np.random.shuffle(data)\n",
        "   \n",
        "    N = x.shape[0]\n",
        "    X_train, y_train = data[:int(N*train_size), :-1], data[:int(N*train_size), -1]\n",
        "    X_test, y_test = data[int(N*train_size):, :-1], data[int(N*train_size):, -1]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "EUgiWLDhUAAK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test= split_data(x,y) # split your data into x_train, x_test, y_train, y_test\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr2Akm_A_FcJ",
        "outputId": "85f7a239-25d6-4f20-d122-e96e1978a8d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800, 3) (800,) (200, 3) (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def covariance(x, mu):\n",
        "\n",
        "  # Easy way: cov= np.cov(x, rowvar=0) but do not use it. One can use it to assess his/her result.\n",
        "  N, D = x.shape\n",
        "  cov_matrix = np.zeros((D, D))\n",
        "  \n",
        "  for i in range(D):\n",
        "\n",
        "    for j in range(D):\n",
        "      # print(x.T[i], x.T[j])\n",
        "      cov_matrix[i][j] = 1/(N-1)* np.sum((x.T[i] - mu[i])@(x.T[j] - mu[j]).T, axis = 0)\n",
        "  return cov_matrix"
      ],
      "metadata": {
        "id": "FrJ7GfXBHNxD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covariance(X_train, [0,0,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVjSXCqb6nIO",
        "outputId": "28085f29-6ee9-427e-943f-f78b70f22cae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.81072111, 0.00351152, 1.02391549],\n",
              "       [0.00351152, 1.94629854, 0.0551522 ],\n",
              "       [1.02391549, 0.0551522 , 1.81263367]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.cov(X_train, rowvar=0 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDreq6Db1fFh",
        "outputId": "5615d3b9-a938-413c-90c8-062ecc4b24c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.81071962, 0.0022824 , 1.02389112],\n",
              "       [0.0022824 , 0.93446696, 0.03508599],\n",
              "       [1.02389112, 0.03508599, 1.81223572]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GDA:\n",
        "  def __init__(self):\n",
        "    ## set mu, phi and sigma to None\n",
        "    self.mu = None\n",
        "    self.phi = None\n",
        "    self.sigma = None\n",
        "    \n",
        "  def fit(self,x,y):\n",
        "    k= len(np.unique(y)) # Number of class.\n",
        "    d= x.shape[1]  # input dim\n",
        "    m= x.shape[0] # Number of examples.\n",
        "    classes = np.unique(y)\n",
        "    \n",
        "    ## Initialize mu, phi and sigma\n",
        "    self.mu= np.zeros((k, d))#: kxd, i.e., each row contains an individual class mu.\n",
        "    self.sigma= np.zeros((k, d, d))#: kxdxd, i.e., each row contains an individual class sigma.\n",
        "    self.phi= np.zeros((d,1))# d-dimension\n",
        "\n",
        "    ## START THE LEARNING: estimate mu, phi and sigma.\n",
        "\n",
        "    for i, elt in enumerate(classes):\n",
        "      idx = [(y == elt)]\n",
        "      N = len(y[idx])\n",
        "      self.phi[i] = N/m\n",
        "      \n",
        "      for j in range(d):\n",
        "        self.mu[i][j] = np.sum((x[idx].T)[j])/ x[idx].shape[0]\n",
        "\n",
        "      self.sigma[i] = covariance(x, self.mu[i])\n",
        "\n",
        "    # return self.mu, self.phi, self.sigma\n",
        "      \n",
        "\n",
        "\n",
        "  def predict_proba(self,x):\n",
        "    # reshape or flatt x.\n",
        "    # x= x.reshape(-1, x.shape[1])\n",
        "    d= x.shape[1]\n",
        "    k_class= self.mu.shape[0] # Number of classes we have in our case it's k = 2\n",
        "    probas = np.zeros((x.shape[0], self.mu.shape[0]))\n",
        "    \n",
        "    ## START THE LEARNING: estimate mu, phi and sigma.\n",
        "\n",
        "    for k in range(k_class):\n",
        "      det = np.linalg.det(self.sigma[k])\n",
        "      inverse = np.linalg.inv(self.sigma[k])\n",
        "\n",
        "      for j in range(len(x)):\n",
        "        probas[j][k] = self.phi[k] * 1.0/(((2*math.pi)**d/2) * (det**0.5)) * np.exp(-0.5* (x[j] - self.mu[k]).T@(inverse)@(x[j] - self.mu[k]))\n",
        "\n",
        "    return probas\n",
        "\n",
        "        \n",
        "\n",
        "  def predict(self,x):\n",
        "    proba = self.predict_proba(x)\n",
        "    k_class= self.mu.shape[0]\n",
        "    y_pred = []\n",
        "\n",
        "    for i in range(proba.shape[0]):\n",
        "      y_pred.append(np.argmax(proba[i]))\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "    \n",
        "  \n",
        "  def accuracy(self, y, ypreds):\n",
        "    pres = np.mean(y==ypreds)*100\n",
        "\n",
        "    return pres"
      ],
      "metadata": {
        "id": "1ocKLDAfceF0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= GDA()\n",
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "l_qO0Yp1c3Is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c0483fd-a698-4ae2-bb92-53622e52e22b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-cbd1cee72bd6>:23: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  N = len(y[idx])\n",
            "<ipython-input-8-cbd1cee72bd6>:27: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  self.mu[i][j] = np.sum((x[idx].T)[j])/ x[idx].shape[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yproba= model.predict_proba(X_test)\n",
        "yproba"
      ],
      "metadata": {
        "id": "NKY1eojY1l4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d8846d-b087-4f6b-c7ee-9d39e2f05b78"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.06859481e-04, 2.30298037e-04],\n",
              "       [1.99679301e-05, 1.17513389e-05],\n",
              "       [4.35037069e-04, 1.32400296e-03],\n",
              "       [3.33652955e-04, 1.95231045e-04],\n",
              "       [1.68285951e-04, 1.21055982e-04],\n",
              "       [1.70455575e-03, 5.50831471e-04],\n",
              "       [5.59392909e-04, 1.89912286e-03],\n",
              "       [6.87528761e-04, 7.34874668e-04],\n",
              "       [1.28507072e-03, 8.61747699e-04],\n",
              "       [1.27611745e-04, 2.82204520e-04],\n",
              "       [8.65954418e-05, 4.81712762e-04],\n",
              "       [5.31291803e-04, 1.86378071e-03],\n",
              "       [1.21142195e-03, 1.66278288e-03],\n",
              "       [4.39032380e-04, 6.11115680e-04],\n",
              "       [7.75071911e-04, 1.84102077e-03],\n",
              "       [5.35214052e-04, 1.23460984e-03],\n",
              "       [9.43794849e-05, 3.21851419e-04],\n",
              "       [7.21240166e-04, 2.10638046e-03],\n",
              "       [6.79393125e-04, 1.65072476e-03],\n",
              "       [3.67770734e-04, 1.47275044e-03],\n",
              "       [2.92669754e-04, 4.15655567e-04],\n",
              "       [8.68405095e-04, 3.07370964e-04],\n",
              "       [1.20667268e-03, 2.02003900e-03],\n",
              "       [2.88992592e-04, 2.05018880e-04],\n",
              "       [1.54522351e-03, 6.01141558e-04],\n",
              "       [3.55626625e-05, 1.75110910e-05],\n",
              "       [4.55130265e-04, 6.54716009e-04],\n",
              "       [1.31385714e-04, 8.32489366e-04],\n",
              "       [1.85633471e-03, 5.72338677e-04],\n",
              "       [1.16081950e-03, 4.92081666e-04],\n",
              "       [6.53434402e-07, 1.58140227e-07],\n",
              "       [3.23912164e-05, 1.40396492e-05],\n",
              "       [1.66247398e-03, 1.76300127e-03],\n",
              "       [1.67416151e-03, 5.93566615e-04],\n",
              "       [1.37938432e-03, 6.94682341e-04],\n",
              "       [2.25742454e-04, 9.19062437e-04],\n",
              "       [3.14070205e-04, 1.62034488e-04],\n",
              "       [1.31264364e-03, 4.96350200e-04],\n",
              "       [2.01562478e-04, 1.41205803e-04],\n",
              "       [1.54492491e-03, 1.35425770e-03],\n",
              "       [1.80693977e-04, 3.96857225e-05],\n",
              "       [2.33907098e-04, 5.64768161e-04],\n",
              "       [3.80866435e-04, 1.42528748e-03],\n",
              "       [2.01177713e-03, 8.43156535e-04],\n",
              "       [9.98782093e-05, 6.26248396e-04],\n",
              "       [3.08717128e-04, 1.13649657e-04],\n",
              "       [1.69932139e-03, 6.72314100e-04],\n",
              "       [1.50570514e-03, 7.39973973e-04],\n",
              "       [6.84804497e-04, 1.79308422e-03],\n",
              "       [3.87489102e-04, 1.39174912e-03],\n",
              "       [7.22040855e-04, 5.32533708e-04],\n",
              "       [9.47278945e-04, 1.66971653e-03],\n",
              "       [1.57543583e-04, 1.41759881e-04],\n",
              "       [9.85299232e-04, 4.87137245e-04],\n",
              "       [3.31847967e-04, 1.32255758e-04],\n",
              "       [5.03472195e-04, 1.63600869e-03],\n",
              "       [1.00334891e-03, 1.58294488e-03],\n",
              "       [7.72685268e-04, 6.24403227e-04],\n",
              "       [1.75789663e-04, 1.64413278e-04],\n",
              "       [7.57863759e-06, 5.42157466e-06],\n",
              "       [2.04713849e-03, 9.12998583e-04],\n",
              "       [4.80954550e-05, 3.25749247e-04],\n",
              "       [1.99163291e-04, 4.23692780e-04],\n",
              "       [1.00016642e-03, 1.26259424e-03],\n",
              "       [1.60034965e-03, 1.79508370e-03],\n",
              "       [5.10240695e-04, 6.23768746e-04],\n",
              "       [4.72466614e-04, 1.15425119e-03],\n",
              "       [1.39180296e-04, 8.64756593e-04],\n",
              "       [3.75579964e-06, 5.59408372e-05],\n",
              "       [5.96374134e-04, 5.82114870e-04],\n",
              "       [5.84211416e-04, 1.45844241e-04],\n",
              "       [2.16547333e-05, 3.00499056e-05],\n",
              "       [5.52708033e-04, 2.98272029e-04],\n",
              "       [9.46567133e-05, 2.76264493e-04],\n",
              "       [1.28393747e-03, 5.56133799e-04],\n",
              "       [5.91473936e-04, 1.75501062e-03],\n",
              "       [7.38915646e-04, 9.43042859e-04],\n",
              "       [8.22449357e-04, 1.10489313e-03],\n",
              "       [3.87790335e-04, 1.07155682e-03],\n",
              "       [3.38305825e-04, 1.08136629e-03],\n",
              "       [9.60286553e-04, 4.59084111e-04],\n",
              "       [1.08913606e-03, 6.63086152e-04],\n",
              "       [2.47070376e-04, 7.95575770e-04],\n",
              "       [1.12743542e-03, 4.18090793e-04],\n",
              "       [1.28937236e-03, 1.82329032e-03],\n",
              "       [3.89523857e-04, 5.43652227e-04],\n",
              "       [3.69037200e-04, 1.64744361e-03],\n",
              "       [6.24440161e-04, 1.17204800e-03],\n",
              "       [6.09164085e-05, 2.61175786e-05],\n",
              "       [7.88715500e-04, 6.78682238e-04],\n",
              "       [1.33853198e-03, 1.44851464e-03],\n",
              "       [1.49539434e-05, 1.39591390e-05],\n",
              "       [1.20396720e-03, 2.01697906e-03],\n",
              "       [9.32199105e-04, 3.56610695e-04],\n",
              "       [2.45959369e-04, 6.86336272e-05],\n",
              "       [1.99977042e-03, 7.66474070e-04],\n",
              "       [6.28705661e-04, 1.34683746e-03],\n",
              "       [1.97040318e-04, 9.43351976e-04],\n",
              "       [1.16192143e-03, 1.47197367e-03],\n",
              "       [1.15485395e-03, 6.20721873e-04],\n",
              "       [5.12453587e-04, 3.18692889e-04],\n",
              "       [6.79250175e-04, 5.32392088e-04],\n",
              "       [2.02153953e-03, 8.72026983e-04],\n",
              "       [9.31779444e-05, 1.95214620e-04],\n",
              "       [1.38842257e-03, 3.94888565e-04],\n",
              "       [1.53681703e-03, 5.96050661e-04],\n",
              "       [5.31058098e-04, 1.43411084e-04],\n",
              "       [1.45484801e-03, 6.23416953e-04],\n",
              "       [4.53624449e-04, 1.87470752e-03],\n",
              "       [6.11568442e-04, 1.77126654e-03],\n",
              "       [7.34716296e-05, 1.87672276e-04],\n",
              "       [7.87454726e-05, 4.63066457e-05],\n",
              "       [5.82326808e-04, 1.68744869e-04],\n",
              "       [1.48046872e-03, 6.47796874e-04],\n",
              "       [7.56626547e-04, 2.98799866e-04],\n",
              "       [1.39373783e-04, 2.88945959e-05],\n",
              "       [4.76579041e-04, 3.31305901e-04],\n",
              "       [1.26528631e-04, 1.06087536e-04],\n",
              "       [1.36523231e-04, 6.81014285e-05],\n",
              "       [9.42227490e-04, 1.55880126e-03],\n",
              "       [1.82860292e-03, 6.88136866e-04],\n",
              "       [7.68507856e-04, 4.30452021e-04],\n",
              "       [1.49051173e-04, 3.49256920e-05],\n",
              "       [6.77216158e-04, 2.06539412e-03],\n",
              "       [5.29093263e-04, 1.69997413e-03],\n",
              "       [1.28830388e-04, 4.61398312e-04],\n",
              "       [3.42225207e-04, 1.90000872e-04],\n",
              "       [1.42365232e-03, 1.86888369e-03],\n",
              "       [4.61773234e-04, 2.21471107e-04],\n",
              "       [4.59442123e-04, 6.88528604e-04],\n",
              "       [1.43877398e-04, 1.73901416e-04],\n",
              "       [2.83759323e-05, 1.70001989e-04],\n",
              "       [9.08584379e-04, 2.46703790e-04],\n",
              "       [6.95116100e-04, 9.57732759e-04],\n",
              "       [2.61603933e-04, 1.48218715e-03],\n",
              "       [5.12364581e-04, 8.42777619e-04],\n",
              "       [2.09646841e-03, 7.71273340e-04],\n",
              "       [3.48872417e-05, 2.33827324e-04],\n",
              "       [4.41339090e-04, 8.58303668e-04],\n",
              "       [1.05106602e-03, 1.51965207e-03],\n",
              "       [3.66789984e-04, 1.39201299e-03],\n",
              "       [1.79152820e-03, 6.75069223e-04],\n",
              "       [1.17115777e-03, 2.99483745e-04],\n",
              "       [4.77192156e-04, 2.24738140e-04],\n",
              "       [7.60862124e-04, 2.24516024e-04],\n",
              "       [2.05871787e-03, 1.01440596e-03],\n",
              "       [5.00711241e-05, 2.87875898e-04],\n",
              "       [4.57264561e-04, 1.85954373e-04],\n",
              "       [4.21547278e-04, 9.28175166e-05],\n",
              "       [1.50337572e-03, 5.87339423e-04],\n",
              "       [5.13545342e-08, 4.82117663e-08],\n",
              "       [3.64613571e-04, 3.76231049e-04],\n",
              "       [1.26696186e-03, 1.52119009e-03],\n",
              "       [2.15952238e-04, 4.05435679e-04],\n",
              "       [9.00223393e-04, 1.15531484e-03],\n",
              "       [7.93974227e-04, 2.27807147e-04],\n",
              "       [1.62468151e-05, 1.54970175e-05],\n",
              "       [1.30463823e-05, 4.22086112e-05],\n",
              "       [6.58229008e-04, 5.47878444e-04],\n",
              "       [6.71369970e-04, 2.03534860e-04],\n",
              "       [9.10845723e-04, 9.12176154e-04],\n",
              "       [3.73905261e-06, 2.12061823e-06],\n",
              "       [2.51709586e-04, 2.37991131e-04],\n",
              "       [1.51309742e-03, 5.23693091e-04],\n",
              "       [3.62140658e-04, 1.26511159e-04],\n",
              "       [4.59547034e-04, 1.26752339e-03],\n",
              "       [4.58939270e-04, 4.13685363e-04],\n",
              "       [1.28460359e-03, 6.67533428e-04],\n",
              "       [6.48537331e-04, 2.08870480e-04],\n",
              "       [9.25378540e-05, 6.04146480e-05],\n",
              "       [4.68640785e-05, 3.55801723e-04],\n",
              "       [9.23746912e-04, 9.18032939e-04],\n",
              "       [9.58640291e-04, 1.04722722e-03],\n",
              "       [1.93267085e-03, 8.67700280e-04],\n",
              "       [1.43851777e-04, 7.76418190e-04],\n",
              "       [4.60683218e-04, 1.32179166e-04],\n",
              "       [1.83983349e-03, 1.03687064e-03],\n",
              "       [6.93482748e-04, 1.14213361e-03],\n",
              "       [1.80006346e-03, 6.87366787e-04],\n",
              "       [1.93123152e-03, 5.79032609e-04],\n",
              "       [2.04373723e-04, 6.91106790e-04],\n",
              "       [5.83050527e-04, 1.79203729e-03],\n",
              "       [9.22730375e-05, 2.41065733e-04],\n",
              "       [6.04015188e-04, 3.33115241e-04],\n",
              "       [9.75133749e-04, 4.22037932e-04],\n",
              "       [1.78534197e-03, 1.08520107e-03],\n",
              "       [4.95461534e-04, 1.77906672e-03],\n",
              "       [6.75218682e-04, 2.73668167e-04],\n",
              "       [8.27343725e-04, 5.87322624e-04],\n",
              "       [1.22492908e-03, 7.53990590e-04],\n",
              "       [6.59291454e-04, 1.62947518e-04],\n",
              "       [1.33175563e-03, 4.75672571e-04],\n",
              "       [2.02145840e-03, 8.11207173e-04],\n",
              "       [3.84064223e-04, 6.32388279e-04],\n",
              "       [8.18502485e-04, 2.10329999e-03],\n",
              "       [2.04556464e-03, 9.90756403e-04],\n",
              "       [2.64799532e-04, 5.73230983e-04],\n",
              "       [2.18870020e-05, 4.04083905e-05],\n",
              "       [5.98139316e-05, 2.32287564e-04],\n",
              "       [4.06013874e-04, 4.51718204e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1LXDxparcjl",
        "outputId": "815a61c4-8083-4b05-e552-ea7bb4ad64fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
              "       1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
              "       0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
              "       1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypreds= model.predict(X_test)\n",
        "ypreds"
      ],
      "metadata": {
        "id": "D4clV6PK1UJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78f9f99-9499-4a70-d037-11dc7630f245"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.accuracy(y_test, ypreds)"
      ],
      "metadata": {
        "id": "QgG1xPUg1ULw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f6e429-70b6-47fd-86f6-33eb17365a27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpXYY-yj1UOj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8cvRcUO2rtKo"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}