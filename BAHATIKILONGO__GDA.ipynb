{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BahatiKilongo/GDA_Live_coding_FML23/blob/main/BAHATIKILONGO__GDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GDA Implementation.\n",
        "\n",
        "Implement the Gaussian Discriminant Analysis (GDA) learning algorithm following the steps as discussed in class.\n",
        "\n",
        "INSTRUCTION: Rename your notebook as: <br>\n",
        "`firstName_LastName_Live_coding_GDA.ipynb`.\n",
        "\n",
        "Notes: \n",
        "* Do not use any built-in functions to complete a task;\n",
        "* Do not import additional libraries."
      ],
      "metadata": {
        "id": "g17Z46tmw2oZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT5nlL-QTKwv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data\n",
        "def generate_data():\n",
        "  x, y = make_classification(n_samples= 1000, n_features=3, n_redundant=0, \n",
        "                           n_informative=3, random_state=1, \n",
        "                           n_clusters_per_class=1)\n",
        "  \n",
        "  return x,y\n",
        "\n",
        "x,y= generate_data() # get data\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-lL4Yq9Tbzn",
        "outputId": "ece0ca68-7150-4c66-c3f7-0e14748083f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 3) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_data(X,y, train_size = 0.8):\n",
        "   \n",
        "  n = int(len(X)*train_size)\n",
        "  indices = np.arange(len(X))\n",
        "  np.random.shuffle(indices)\n",
        "  train_idx = indices[: n]\n",
        "  test_idx = indices[n:]\n",
        "  X_train, y_train = X[train_idx], y[train_idx]\n",
        "  X_test, y_test = X[test_idx], y[test_idx]\n",
        "\n",
        "  return X_train, y_train, X_test, y_test\n"
      ],
      "metadata": {
        "id": "EUgiWLDhUAAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,y_train, X_test, y_test= split_data(x,y) # split your data into x_train, x_test, y_train, y_test\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr2Akm_A_FcJ",
        "outputId": "6967d220-79cc-43b2-f2c9-479c9304413c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800, 3) (800,) (200, 3) (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(np.unique(y_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8nU_YdNbZFj",
        "outputId": "961b23e5-88c2-4070-bf53-c50301568566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def covariance(x, mu):\n",
        "  mu=np.mean(x, axis=0)\n",
        "  n,d=x.shape\n",
        "  cov=np.zeros((d,d))\n",
        "  #print(cov.shape)\n",
        "  # Easy way: cov= np.cov(x, rowvar=0) but do not use it. One can use it to assess his/her result.\n",
        "  for i in range(d):\n",
        "    for j in range(d):\n",
        "      cov[i,j]=np.sum((x[:,i]-mu[i])*(x[:,j]-mu[j]))/(len(x)-1)\n",
        "  return cov"
      ],
      "metadata": {
        "id": "bDapQULQ_9O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu=np.mean(X_test, axis=0)\n",
        "covariance(X_test,mu ) - np.cov(X_test, rowvar=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bliOfKwz0dvw",
        "outputId": "70e2ae9c-b7dd-42fe-8194-148449b2e7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.55431223e-15,  0.00000000e+00,  4.44089210e-16],\n",
              "       [ 0.00000000e+00,  2.22044605e-16, -1.56125113e-17],\n",
              "       [ 4.44089210e-16, -1.56125113e-17,  1.11022302e-15]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GDA:\n",
        "  def __init__(self):\n",
        "    ## set mu, phi and sigma to None\n",
        "    self.mu = None\n",
        "    self.phi = None\n",
        "    self.sigma = None\n",
        "    \n",
        "  def fit(self,x,y):\n",
        "    K = len(np.unique(y.reshape(-1))) # Number of class.\n",
        "    d= x.shape[1]  # input dim\n",
        "    m= x.shape[0] # Number of examples.\n",
        "    #x = x.reshape(m,-1)\n",
        "\n",
        "    \n",
        "    \n",
        "    ## Initialize mu, phi and sigma\n",
        "    self.mu= np.zeros((K,d))       #: kxd, i.e., each row contains an individual class mu.\n",
        "    self.sigma= np.zeros((K,d,d))  #: kxdxd, i.e., each row contains an individual class sigma.\n",
        "    self.phi= np.zeros((K,1))      # k-dimension\n",
        "\n",
        "    ## START THE LEARNING: estimate mu, phi and sigma.\n",
        "    for i in range(K):\n",
        "      class_ind = (i == y)\n",
        "      #self.mu[i] = np.mean(x[class_ind,:], axis=0)\n",
        "      self.mu[i] = np.sum(x[y == i]) *x[i] /np.sum(x[class_ind])\n",
        "      self.sigma[i] = covariance(x[y==i],self.mu[i])\n",
        "      self.phi[i] = (np.sum(y==1)) / m\n",
        "    return self.mu, self.sigma, self.phi\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "  def predict_proba(self,x):\n",
        "    \n",
        "    # reshape or flatt x.\n",
        "    #x= np.reshape(-1,1)\n",
        "    d= x.shape[0]\n",
        "    K =self.mu.shape[0] # Number of classes we have in our case it's k = 2\n",
        "    proba=np.zeros((d,K))\n",
        "    \n",
        "    ## START THE LEARNING: estimate mu, phi and sigma.\n",
        "    for i in range(K):\n",
        "      sigma_det=np.linalg.det(self.sigma[i])\n",
        "      sigma_inv=np.linalg.inv(self.sigma[i])\n",
        "      for j in range(x.shape[0]):\n",
        "        fact1 =1/((2*np.pi)**(d/2)*(sigma_det**0.5))\n",
        "        fact2 =np.exp( -0.5*((x[j]-self.mu[i]).T)@(sigma_inv)@(x[j]-self.mu[i]) )\n",
        "        gaussian=np.exp(fact2)\n",
        "        proba[j, i]= gaussian*self.phi[i]\n",
        "    return proba\n",
        "\n",
        "  def predict(self,x):\n",
        "    pred=self.predict_proba(x)\n",
        "    ypred=np.argmax(pred, axis=1)\n",
        "    return ypred\n",
        "  \n",
        "  def accuracy(self, y, ypreds):\n",
        "    reslt=np.mean(y==ypreds)\n",
        "    return reslt*100"
      ],
      "metadata": {
        "id": "1ocKLDAfceF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GDA()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp8rwYL89p1I",
        "outputId": "f5690779-fc8b-49df-8623-60d449d9379b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 1.98002276,  0.12186164,  0.63331114],\n",
              "        [ 1.83390275, -1.06230907,  1.12266418]]),\n",
              " array([[[ 0.86158965, -0.33611909, -0.05787815],\n",
              "         [-0.33611909,  1.6472537 ,  0.09882649],\n",
              "         [-0.05787815,  0.09882649,  0.03919141]],\n",
              " \n",
              "        [[ 0.79704476,  0.34257941,  0.09802025],\n",
              "         [ 0.34257941,  0.35206826, -0.07220905],\n",
              "         [ 0.09802025, -0.07220905,  1.56433587]]]),\n",
              " array([[0.505],\n",
              "        [0.505]]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yproba = model.predict_proba(X_test)\n",
        "yproba"
      ],
      "metadata": {
        "id": "NKY1eojY1l4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6aa4171-6934-4f7b-a3a2-66e0be3f5fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.50566457, 0.50500005],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.50733371, 0.505     ],\n",
              "       [0.5663933 , 0.50500689],\n",
              "       [0.78695532, 0.50645439],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.58236906, 0.505     ],\n",
              "       [0.50500126, 0.50507579],\n",
              "       [0.50579847, 0.505     ],\n",
              "       [0.52337307, 0.505     ],\n",
              "       [0.63324032, 0.505796  ],\n",
              "       [1.142297  , 0.50515808],\n",
              "       [0.59300257, 0.7691339 ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.51433578, 0.505     ],\n",
              "       [0.50500347, 0.505     ],\n",
              "       [0.50510796, 0.505     ],\n",
              "       [0.87118862, 0.56699166],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.58535932, 0.505     ],\n",
              "       [0.50913361, 0.505     ],\n",
              "       [0.5326869 , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.59941135, 0.7239743 ],\n",
              "       [0.51235599, 0.50506219],\n",
              "       [1.08384886, 0.50500533],\n",
              "       [0.60857803, 0.5479582 ],\n",
              "       [0.54837075, 0.505     ],\n",
              "       [0.63237028, 0.50551579],\n",
              "       [0.58983806, 0.51423492],\n",
              "       [0.52860576, 0.50532303],\n",
              "       [0.51127055, 0.772303  ],\n",
              "       [0.50501857, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.53495103, 0.505     ],\n",
              "       [0.50593691, 0.505     ],\n",
              "       [0.50890646, 0.80542049],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.50683358, 0.50500107],\n",
              "       [0.7552498 , 0.57850172],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.581642  , 0.505     ],\n",
              "       [0.52378188, 0.70599172],\n",
              "       [0.50751366, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.51369706, 0.50500122],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.51009791, 0.50503605],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.81764412, 0.50500015],\n",
              "       [0.51148215, 0.505     ],\n",
              "       [0.5958347 , 0.5052056 ],\n",
              "       [0.82159778, 0.5611548 ],\n",
              "       [0.5385128 , 0.5053499 ],\n",
              "       [0.50734945, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.50640042, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.89633809, 0.50500004],\n",
              "       [0.50738635, 0.505     ],\n",
              "       [0.70262487, 0.51923655],\n",
              "       [0.61058915, 0.50500078],\n",
              "       [0.51054231, 0.50500001],\n",
              "       [0.67928567, 0.50535894],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.62027924, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.5065351 , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.58020101, 0.50710385],\n",
              "       [0.51882428, 0.505     ],\n",
              "       [0.50984083, 0.505     ],\n",
              "       [0.50544946, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.52090529, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.56561481, 1.09256772],\n",
              "       [0.50500002, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [1.12439987, 0.5211465 ],\n",
              "       [0.50564459, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.50500951, 0.505     ],\n",
              "       [0.51744573, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.56956535, 0.50501983],\n",
              "       [0.50500292, 0.505     ],\n",
              "       [0.83758595, 1.01744966],\n",
              "       [0.505     , 0.505     ],\n",
              "       [1.11382532, 0.50502063],\n",
              "       [0.98217879, 0.50562718],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.50500008, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.66259063, 0.86390471],\n",
              "       [0.53751379, 0.5198494 ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.54880099, 1.17080396],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.50751002, 0.505     ],\n",
              "       [0.5073016 , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.50730724, 0.50500011],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.76190797, 0.52209105],\n",
              "       [1.01093869, 0.50507078],\n",
              "       [0.587642  , 0.505     ],\n",
              "       [0.78189083, 0.50603063],\n",
              "       [0.50500368, 0.505     ],\n",
              "       [0.69009673, 0.5050001 ],\n",
              "       [0.50528807, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.50501692, 0.505     ],\n",
              "       [0.52692056, 0.68097327],\n",
              "       [0.98017781, 0.50512564],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.51554204, 0.505     ],\n",
              "       [0.50562239, 0.505     ],\n",
              "       [0.50525557, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.6186111 , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.54133995, 0.50500701],\n",
              "       [0.50801989, 0.505     ],\n",
              "       [0.50943421, 0.505     ],\n",
              "       [0.50501852, 0.505     ],\n",
              "       [1.07942975, 0.50854916],\n",
              "       [0.70463352, 0.50500003],\n",
              "       [0.51323394, 0.50509739],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.54627018, 0.50502721],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.55026706, 0.50500001],\n",
              "       [0.5154885 , 1.22163636],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.82352483, 0.50500043],\n",
              "       [0.88507206, 0.5050187 ],\n",
              "       [0.57611035, 0.50500065],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.72487768, 0.60287073],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.53391823, 0.505     ],\n",
              "       [0.50607367, 0.50500133],\n",
              "       [0.50500001, 0.505     ],\n",
              "       [0.50500017, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.50758293, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.51093576, 0.505     ],\n",
              "       [0.52504245, 0.50506102],\n",
              "       [0.50500716, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.55348527, 0.52273344],\n",
              "       [0.50501938, 0.505     ],\n",
              "       [0.520046  , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.52485278, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.50500003, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.5982475 , 0.51557247],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.51658884, 0.505     ],\n",
              "       [0.51148285, 0.505     ],\n",
              "       [0.50567634, 0.505     ],\n",
              "       [0.56300544, 0.50500001],\n",
              "       [0.62692991, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.53028547, 0.505     ],\n",
              "       [0.50500171, 0.505     ],\n",
              "       [0.505     , 0.505     ],\n",
              "       [0.7362183 , 0.71817604]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypreds= model.predict(X_test)\n",
        "ypreds\n",
        "\n"
      ],
      "metadata": {
        "id": "D4clV6PK1UJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aefbc241-442a-40a5-ad47-b8f6f6bbec87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
              "       0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.accuracy(y_test, ypreds)"
      ],
      "metadata": {
        "id": "QgG1xPUg1ULw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3b1836-e0d6-4ae1-80c5-39aaa8963520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ],
      "metadata": {
        "id": "XowJhRjpm-X5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "  '''\n",
        "  The goal of this class is to create a LogisticRegression class, \n",
        "  that we will use as our model to classify data point into a corresponding class\n",
        "  '''\n",
        "  def __init__(self,lr,n_epochs):\n",
        "    self.lr = lr\n",
        "    self.n_epochs = n_epochs\n",
        "    self.train_losses = []\n",
        "    self.w = None\n",
        "    self.weight = []\n",
        "\n",
        "  def add_ones(self, x):\n",
        "    ##### WRITE YOUR CODE HERE #####\n",
        "    one = np.ones((x.shape[0],1))\n",
        "    return np.hstack((one,x))\n",
        "    #### END CODE ####\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1/(1+np.exp(-x@self.w))\n",
        "\n",
        "\n",
        "  def cross_entropy(self, x, y_true):\n",
        "    ##### WRITE YOUR CODE HERE #####\n",
        "    y_pred = self.sigmoid(x)\n",
        "    loss = -np.mean(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))\n",
        "    return loss\n",
        "    #### END CODE ####\n",
        "  \n",
        "  def predict_proba(self,x):  #This function will use the sigmoid function to compute the probalities\n",
        "    ##### WRITE YOUR CODE HERE #####\n",
        "    x= self.add_ones(x)\n",
        "    proba = self.sigmoid(x)\n",
        "    return proba\n",
        "    #### END CODE ####\n",
        "\n",
        "  def predict(self,x):\n",
        "    ##### WRITE YOUR CODE HERE #####\n",
        "    probas = self.predict_proba(x)\n",
        "    output = [0 if p<0.5 else 1 for p in probas]#np.where(probas>=0.5, 1, 0)      #convert the probalities into 0 and 1 by using a treshold=0.5\n",
        "    return output\n",
        "    #### END CODE ####\n",
        "\n",
        "  def fit(self,x,y):\n",
        "    # Add ones to x\n",
        "    x=self.add_ones(x)\n",
        "\n",
        "    # reshape y if needed\n",
        "    y=y.reshape(-1,1)\n",
        "\n",
        "    # Initialize w to zeros vector >>> (x.shape[1])\n",
        "    self.w=np.zeros((x.shape[1],1))\n",
        "\n",
        "    for epoch in range(self.n_epochs):\n",
        "      # make predictions\n",
        "      ypred = self.sigmoid(x)\n",
        "\n",
        "      #compute the gradient\n",
        "      dl = (-1/x.shape[0])*(x.T@(y-ypred))\n",
        "\n",
        "      #update rule\n",
        "      self.w=self.w-self.lr*dl\n",
        "\n",
        "      #Compute and append the training loss in a list\n",
        "      loss = self.cross_entropy(x,y)\n",
        "      self.train_losses.append(loss)\n",
        "\n",
        "      if epoch%1000 == 0:\n",
        "        print(f'loss for epoch {epoch}  : {loss}')\n",
        "\n",
        "  def accuracy(self,y_true, y_pred):\n",
        "    ##### WRITE YOUR CODE HERE #####\n",
        "    acc = np.mean(y_true==y_pred)*100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "OpXYY-yj1UOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(0.01,n_epochs=10000)\n",
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "8cvRcUO2rtKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a350ef70-ed01-40ef-c025-d53785331e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss for epoch 0  : 0.6883420100966741\n",
            "loss for epoch 1000  : 0.19939132988838457\n",
            "loss for epoch 2000  : 0.1751950169548517\n",
            "loss for epoch 3000  : 0.16616634669872749\n",
            "loss for epoch 4000  : 0.16131500670259905\n",
            "loss for epoch 5000  : 0.15822537170483092\n",
            "loss for epoch 6000  : 0.15605733636588565\n",
            "loss for epoch 7000  : 0.15444091528407455\n",
            "loss for epoch 8000  : 0.15318651020708832\n",
            "loss for epoch 9000  : 0.15218580544708607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypred_train = model.predict(X_train)\n",
        "acc = model.accuracy(y_train,ypred_train)\n",
        "print(f\"The training accuracy is: {acc}\")\n",
        "print(\" \")\n",
        "\n",
        "ypred_test = model.predict(X_test)\n",
        "acc = model.accuracy(y_test,ypred_test)\n",
        "print(f\"The test accuracy is: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pvRjMQ4oALX",
        "outputId": "4b8682e0-a3ee-4919-a49a-9287e5cc1064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training accuracy is: 50.005937499999995\n",
            " \n",
            "The test accuracy is: 49.985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EoVwAgQdoKR4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}