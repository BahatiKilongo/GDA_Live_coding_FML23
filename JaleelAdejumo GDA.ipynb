{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GDA Implementation.\n",
        "\n",
        "Implement the Gaussian Discriminant Analysis (GDA) learning algorithm following the steps as discussed in class.\n",
        "\n",
        "INSTRUCTION: Rename your notebook as: <br>\n",
        "`firstName_LastName_Live_coding_GDA.ipynb`.\n",
        "\n",
        "Notes: \n",
        "* Do not use any built-in functions to complete a task;\n",
        "* Do not import additional libraries."
      ],
      "metadata": {
        "id": "g17Z46tmw2oZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aT5nlL-QTKwv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data\n",
        "def generate_data():\n",
        "  x, y = make_classification(n_samples= 1000, n_features=3, n_redundant=0, \n",
        "                           n_informative=3, random_state=1, \n",
        "                           n_clusters_per_class=1)\n",
        "  \n",
        "  return x,y\n",
        "\n",
        "x,y= generate_data()\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-lL4Yq9Tbzn",
        "outputId": "b4d541ad-f178-426e-e2b1-102c2d49bf3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 3) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(X,y, train_size= 0.8):\n",
        "    # shuffle the data to randomize the train/test split\n",
        "    \n",
        "  np.random.seed(0) # To demonstrate that if we use the same seed value twice, we will get the same random number twice\n",
        "  n = int(len(X)*train_size)\n",
        "  indices = np.arange(len(X))\n",
        "  np.random.shuffle(indices)\n",
        "  train_idx = indices[: n]\n",
        "  test_idx = indices[n:]\n",
        "  X_train, y_train = X[train_idx], y[train_idx]\n",
        "  X_test, y_test = X[test_idx], y[test_idx]\n",
        "  \n",
        "  return X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "EUgiWLDhUAAK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test= split_data(x,y, train_size= 0.8) # split your data into x_train, x_test, y_train, y_test\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr2Akm_A_FcJ",
        "outputId": "9b16a7fb-c520-470c-f991-a27f827696e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800, 3) (800,) (200, 3) (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def covariance(x, mu):\n",
        "  \n",
        "  \n",
        "  cov = ((x-mu).T@(x-mu))/(x.shape[0]-1)\n",
        "  \n",
        "\n",
        "  # Easy way: cov= np.cov(x, rowvar=0) but do not use it. One can use it to assess his/her result.\n",
        "  return cov"
      ],
      "metadata": {
        "id": "FrJ7GfXBHNxD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covariance(X_train,np.mean(X_train,axis = 0))"
      ],
      "metadata": {
        "id": "sDB_RHAr5kp9",
        "outputId": "7152583b-16e7-4e6e-b092-0bffec413379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.81780125, 0.00278495, 1.00021288],\n",
              "       [0.00278495, 0.98802231, 0.04507526],\n",
              "       [1.00021288, 0.04507526, 1.73006042]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cov = np.zeros((X_train.shape[1],X_train.shape[1]))\n",
        "for i in range(X_train.shape[1]):\n",
        "  mu_i = np.mean(X_train.T[i])\n",
        "  for j in range(X_train.shape[1]):\n",
        "    mu_j = np.mean(X_train.T[j])\n",
        "    cov[i,j] = (np.sum(i-mu_i*j-mu_j))/(X_train.shape[0]-1)\n",
        "\n",
        "print(cov)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6FOO_D4kyHFH",
        "outputId": "d124d1c5-ec24-41b7-f700-52abb96d0218",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-4.94098787e-05 -1.31261365e-03 -1.73998842e-04]\n",
            " [ 1.20215458e-03 -1.27484309e-03 -1.35002218e-03]\n",
            " [ 2.45371903e-03  1.16474605e-03  2.27759166e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(X_train,axis = 0)"
      ],
      "metadata": {
        "id": "EIJlatAF2J-W",
        "outputId": "ab10a127-9b58-45a6-b1d8-1310fae8057b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03947849, 1.00929982, 0.06006809])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cov= np.cov(X_train,rowvar=0)\n",
        "cov"
      ],
      "metadata": {
        "id": "RyxL5Ze3xS1t",
        "outputId": "6477f11a-b70f-4b86-baac-80b4994390ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.81780125, 0.00278495, 1.00021288],\n",
              "       [0.00278495, 0.98802231, 0.04507526],\n",
              "       [1.00021288, 0.04507526, 1.73006042]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GDA:\n",
        "  def __init__(self):\n",
        "    ## set mu, phi and sigma to None\n",
        "    self.mu = None\n",
        "    self.phi = None\n",
        "    self.sigma =None\n",
        "    \n",
        "  def fit(self,x,y):\n",
        "    k= len(np.unique(y))# Number of class.\n",
        "    d = x.shape[1]  # input dim\n",
        "    m=  x.shape[0]# Number of examples.\n",
        "    \n",
        "    ## Initialize mu, phi and sigma\n",
        "    self.mu= np.zeros((k,d))#: kxd, i.e., each row contains an individual class mu.\n",
        "    self.sigma= np.zeros((k,d,d))#: kxdxd, i.e., each row contains an individual class sigma.\n",
        "    self.phi= np.zeros(k)# d-dimension\n",
        "\n",
        "    ## START THE LEARNING: estimate mu, phi and sigma.\n",
        "    for i in range(k):\n",
        "      self.phi[i] = len(y[y==i])/x.shape[0]\n",
        "      self.mu[i] = np.mean(x[y==i],axis = 0)\n",
        "      self.sigma[i] = covariance(x[y==i],self.mu[i])\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  def predict_proba(self,x):\n",
        "    # reshape or flatt x.\n",
        "    #x= x\n",
        "    d= x.shape[1]\n",
        "    k= self.mu.shape[0]\n",
        "    k_class = np.zeros((x.shape[0],k))\n",
        "    constant = (2*np.pi)**(d/2)\n",
        "    for i in range(k):\n",
        "      for j in range(x.shape[0]):\n",
        "        k_class[j,i]= np.exp(-0.5*((x[j]-self.mu[i]).T@np.linalg.inv(self.sigma[i])@(x[j]-self.mu[i]).T))/(constant*(np.linalg.det(self.sigma[i])**0.5))\n",
        "        #np.exp(-0.5*((x[j]-self.mu[i]).T@np.linalg.inv(self.sigma)@(x[j]-self.mu[i]).T))/(constant*(np.linalg.det(self.sigma[i])**0.5)) # Number of classes we have in our case it's k = 2\n",
        "    return k_class\n",
        "    ## START THE LEARNING: estimate mu, phi and sigma.\n",
        "\n",
        "  def predict(self,x):\n",
        "    predict = self.predict_proba(x) * self.phi\n",
        "    y = np.argmax(predict, axis =1)\n",
        "\n",
        "    #y = np.where(predict[:,0]>predict[:,1],0,1)\n",
        "    return y\n",
        "\n",
        "    \n",
        "  \n",
        "  def accuracy(self, y, ypreds):\n",
        "    acc = np.mean(y == ypreds)\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "1ocKLDAfceF0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= GDA()\n",
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "l_qO0Yp1c3Is"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yproba= model.predict_proba(X_test)\n",
        "yproba"
      ],
      "metadata": {
        "id": "NKY1eojY1l4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3539db65-e209-4e09-add8-8aa18a511bdc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.15206101e-070, 5.76116478e-002],\n",
              "       [4.65661320e-021, 1.96407370e-002],\n",
              "       [1.26397479e-001, 2.50138981e-004],\n",
              "       [1.50802026e-092, 4.55699996e-002],\n",
              "       [1.94935492e-105, 3.28718488e-002],\n",
              "       [2.16014250e-053, 1.14204868e-001],\n",
              "       [1.75720703e-001, 1.51958419e-002],\n",
              "       [8.63916128e-013, 9.95302009e-002],\n",
              "       [2.62908089e-001, 1.04141218e-005],\n",
              "       [1.38640584e-004, 3.13742261e-002],\n",
              "       [1.35545567e-064, 1.21383501e-002],\n",
              "       [5.27411638e-016, 5.24941943e-002],\n",
              "       [1.88325934e-036, 1.14483512e-002],\n",
              "       [1.06667981e-016, 2.09085891e-002],\n",
              "       [2.51835475e-002, 1.91393407e-008],\n",
              "       [2.40580939e-070, 3.37322856e-002],\n",
              "       [4.02488598e-051, 5.85473852e-004],\n",
              "       [4.27261386e-057, 1.10655762e-001],\n",
              "       [2.89469546e-008, 2.77792162e-002],\n",
              "       [3.58551047e-061, 9.74708617e-002],\n",
              "       [1.52075882e-088, 1.73417555e-002],\n",
              "       [2.06884411e-001, 2.66507914e-003],\n",
              "       [1.09924114e-059, 7.40307971e-002],\n",
              "       [1.11657353e-027, 2.32721506e-002],\n",
              "       [1.36278119e-001, 3.37492650e-004],\n",
              "       [8.54918567e-058, 1.56716357e-002],\n",
              "       [6.78931179e-027, 1.10450258e-001],\n",
              "       [1.18430506e-001, 2.31367148e-006],\n",
              "       [4.22320003e-014, 5.10287434e-002],\n",
              "       [1.14558699e-001, 8.01057388e-003],\n",
              "       [8.12937363e-002, 1.16798496e-003],\n",
              "       [9.28481952e-188, 3.29219071e-003],\n",
              "       [2.24944736e-022, 1.14219488e-001],\n",
              "       [7.90151132e-039, 6.24260182e-002],\n",
              "       [4.40379185e-004, 1.05348065e-002],\n",
              "       [3.89605225e-003, 3.38174978e-005],\n",
              "       [3.62976498e-009, 3.68532669e-004],\n",
              "       [8.79227922e-030, 8.57085770e-002],\n",
              "       [5.42692542e-013, 8.42889423e-002],\n",
              "       [9.55319104e-060, 3.93024765e-002],\n",
              "       [2.13842945e-078, 5.89234416e-002],\n",
              "       [5.87596078e-002, 2.46010903e-003],\n",
              "       [7.02763320e-003, 4.58677369e-004],\n",
              "       [5.84054207e-093, 2.35285967e-002],\n",
              "       [1.13990774e-002, 5.49281530e-007],\n",
              "       [8.56161384e-003, 6.18880258e-010],\n",
              "       [6.05582860e-002, 1.17578557e-005],\n",
              "       [7.99647656e-002, 7.46560560e-005],\n",
              "       [1.63719304e-018, 1.88978639e-003],\n",
              "       [2.66132884e-001, 6.68915563e-003],\n",
              "       [3.92624314e-002, 8.79941753e-003],\n",
              "       [2.94342922e-001, 1.73513324e-005],\n",
              "       [1.54017075e-086, 6.52617079e-002],\n",
              "       [4.64381175e-002, 1.33012276e-004],\n",
              "       [3.11892249e-009, 3.15210194e-002],\n",
              "       [3.09289728e-001, 1.75044529e-004],\n",
              "       [6.13278684e-002, 1.33070850e-003],\n",
              "       [1.81972015e-005, 6.78180091e-002],\n",
              "       [7.95085139e-002, 1.53240698e-003],\n",
              "       [2.43270616e-001, 5.01169228e-004],\n",
              "       [4.41600565e-020, 2.08558179e-002],\n",
              "       [1.63521112e-001, 2.20462197e-002],\n",
              "       [1.74619574e-002, 6.67317828e-007],\n",
              "       [4.34978913e-002, 3.15339022e-010],\n",
              "       [1.54868349e-034, 2.32088316e-002],\n",
              "       [4.95410308e-002, 1.81777556e-009],\n",
              "       [9.60081825e-028, 8.42752977e-002],\n",
              "       [9.64825960e-027, 8.96210848e-002],\n",
              "       [1.30436430e-001, 7.64005245e-004],\n",
              "       [1.03435728e-002, 1.86940359e-014],\n",
              "       [1.52072528e-001, 2.95408464e-005],\n",
              "       [2.42048315e-001, 4.60905256e-004],\n",
              "       [1.38833330e-001, 6.70833263e-003],\n",
              "       [4.51556694e-012, 8.81394056e-002],\n",
              "       [5.00357037e-052, 9.19586888e-002],\n",
              "       [5.75410877e-078, 5.09888956e-002],\n",
              "       [5.32363279e-002, 1.65004220e-003],\n",
              "       [1.15675424e-001, 1.29641552e-003],\n",
              "       [7.42726247e-070, 6.44553697e-002],\n",
              "       [2.29384691e-001, 6.94115304e-003],\n",
              "       [1.13953925e-021, 2.45363591e-002],\n",
              "       [3.25872967e-018, 5.58314328e-003],\n",
              "       [2.76364297e-002, 2.44880310e-002],\n",
              "       [2.44541411e-002, 3.01293583e-012],\n",
              "       [5.42677838e-015, 1.07304640e-001],\n",
              "       [2.40621534e-073, 5.85155208e-002],\n",
              "       [1.04574346e-061, 8.81183056e-002],\n",
              "       [4.22250144e-026, 4.03529561e-002],\n",
              "       [4.55442972e-002, 3.72466176e-002],\n",
              "       [8.76702393e-011, 1.97278644e-003],\n",
              "       [8.58359050e-166, 2.91871637e-003],\n",
              "       [3.99022044e-004, 1.45127267e-002],\n",
              "       [5.49581957e-054, 1.04301053e-001],\n",
              "       [2.16779456e-003, 5.05428102e-016],\n",
              "       [2.50964592e-003, 1.36954594e-018],\n",
              "       [1.63364864e-001, 2.19346697e-003],\n",
              "       [2.58335482e-001, 7.07807944e-003],\n",
              "       [2.68878190e-002, 2.27692204e-012],\n",
              "       [2.42259094e-044, 8.24618262e-002],\n",
              "       [3.75774850e-002, 2.48032728e-006],\n",
              "       [9.77507160e-029, 9.76243467e-002],\n",
              "       [2.30706886e-002, 5.78467624e-006],\n",
              "       [3.83616736e-050, 4.19612043e-002],\n",
              "       [3.12066904e-002, 2.23589687e-006],\n",
              "       [1.46237655e-041, 6.65621918e-002],\n",
              "       [7.71408436e-002, 1.65000403e-004],\n",
              "       [7.81550805e-004, 2.25856561e-002],\n",
              "       [2.23205000e-002, 2.91591787e-006],\n",
              "       [2.00873144e-001, 9.19824975e-005],\n",
              "       [3.09349356e-095, 1.83475351e-002],\n",
              "       [9.83141093e-100, 3.07302456e-002],\n",
              "       [9.30005225e-009, 1.30591504e-003],\n",
              "       [4.56860159e-058, 3.62003800e-002],\n",
              "       [3.66956809e-036, 8.03192410e-002],\n",
              "       [1.28016936e-011, 9.15383806e-002],\n",
              "       [3.14925554e-001, 4.50817739e-003],\n",
              "       [8.19847444e-050, 6.35224056e-002],\n",
              "       [1.29095855e-001, 9.96285602e-004],\n",
              "       [2.06483193e-001, 6.39917591e-008],\n",
              "       [3.14023048e-006, 7.80106267e-002],\n",
              "       [2.39320906e-001, 7.45055192e-003],\n",
              "       [3.92895892e-002, 3.13119260e-005],\n",
              "       [1.69857383e-002, 1.60457593e-007],\n",
              "       [3.23681169e-158, 1.57501122e-002],\n",
              "       [1.82480473e-006, 3.89295715e-002],\n",
              "       [6.07162261e-006, 1.51661003e-002],\n",
              "       [1.59147448e-001, 1.18477739e-002],\n",
              "       [2.55648427e-191, 8.74018215e-004],\n",
              "       [1.67725029e-002, 1.85078485e-016],\n",
              "       [5.81893907e-032, 1.18507866e-001],\n",
              "       [8.00024233e-009, 8.07736728e-002],\n",
              "       [3.24790531e-003, 3.44697763e-002],\n",
              "       [6.64104251e-002, 1.28079087e-003],\n",
              "       [7.57145340e-002, 1.83063986e-014],\n",
              "       [7.59381336e-002, 1.53071710e-008],\n",
              "       [1.15261158e-039, 8.32451732e-002],\n",
              "       [4.75437148e-057, 6.00994358e-003],\n",
              "       [1.68901813e-001, 2.04826408e-007],\n",
              "       [2.44553215e-003, 1.40953340e-002],\n",
              "       [2.93447978e-010, 7.04073061e-003],\n",
              "       [1.57812389e-005, 6.84563023e-003],\n",
              "       [5.40433435e-011, 7.15199071e-002],\n",
              "       [7.37339179e-002, 8.51725077e-003],\n",
              "       [5.35841783e-034, 6.36315145e-003],\n",
              "       [2.11701015e-132, 2.97857905e-002],\n",
              "       [1.78840180e-002, 4.64795954e-002],\n",
              "       [2.19119452e-035, 2.68560367e-002],\n",
              "       [3.63090209e-003, 4.82321528e-002],\n",
              "       [9.11365682e-025, 2.48883850e-002],\n",
              "       [3.68571859e-004, 4.07778713e-002],\n",
              "       [1.00972874e-001, 5.79151424e-004],\n",
              "       [7.38333317e-003, 4.33655308e-002],\n",
              "       [9.90069222e-074, 5.46781275e-002],\n",
              "       [4.32614896e-018, 5.43380734e-002],\n",
              "       [3.04800951e-001, 2.94647252e-003],\n",
              "       [5.60966249e-145, 5.37068604e-003],\n",
              "       [4.75323970e-008, 3.06891717e-002],\n",
              "       [9.55111527e-020, 6.18778086e-002],\n",
              "       [1.64450703e-001, 5.72408478e-004],\n",
              "       [3.02759755e-001, 1.37396200e-004],\n",
              "       [2.39373572e-001, 1.31864622e-003],\n",
              "       [1.22387018e-002, 6.32691495e-021],\n",
              "       [1.03557354e-027, 3.31064859e-002],\n",
              "       [1.48233047e-001, 8.34851436e-005],\n",
              "       [5.11401995e-002, 5.42473035e-006],\n",
              "       [1.48643093e-001, 1.58944023e-009],\n",
              "       [6.17229368e-002, 9.80968547e-006],\n",
              "       [2.62533292e-001, 1.12938228e-002],\n",
              "       [1.19655138e-001, 6.00947081e-008],\n",
              "       [4.54290773e-030, 2.83183104e-002],\n",
              "       [9.94830464e-002, 2.72543919e-003],\n",
              "       [1.34593857e-062, 5.17716394e-002],\n",
              "       [3.63222825e-002, 1.01473142e-004],\n",
              "       [5.89753149e-003, 9.22163510e-011],\n",
              "       [9.97402034e-003, 2.10624182e-013],\n",
              "       [3.00366616e-034, 4.78460451e-002],\n",
              "       [1.93893429e-062, 2.94620184e-002],\n",
              "       [1.03942216e-002, 2.01538166e-003],\n",
              "       [1.57905171e-012, 7.12291057e-003],\n",
              "       [1.08088034e-032, 5.13598832e-002],\n",
              "       [6.93695773e-025, 1.95512391e-002],\n",
              "       [1.08690732e-003, 8.09333871e-022],\n",
              "       [8.47190053e-003, 7.70549547e-007],\n",
              "       [2.44623899e-034, 1.12883911e-001],\n",
              "       [5.66373316e-004, 4.88474823e-003],\n",
              "       [7.94875102e-025, 5.80584661e-002],\n",
              "       [1.88168033e-167, 1.01834472e-002],\n",
              "       [2.68153152e-069, 8.32762602e-002],\n",
              "       [1.87317832e-001, 4.70975672e-006],\n",
              "       [3.40683354e-002, 2.31635651e-005],\n",
              "       [5.01252763e-002, 3.76413252e-004],\n",
              "       [1.66641700e-076, 7.89755119e-002],\n",
              "       [8.76778876e-002, 3.29867683e-004],\n",
              "       [1.77547574e-039, 3.47208334e-002],\n",
              "       [1.82108666e-099, 8.52918955e-003],\n",
              "       [1.61927273e-001, 7.70925761e-004],\n",
              "       [5.52395653e-103, 3.14583085e-002],\n",
              "       [2.92158229e-024, 1.80664258e-002],\n",
              "       [8.24650838e-095, 2.58540154e-002],\n",
              "       [3.50166787e-003, 1.08635004e-024]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypreds= model.predict(X_test)\n",
        "ypreds\n"
      ],
      "metadata": {
        "id": "D4clV6PK1UJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57958ab0-0be1-4f74-f258-850720011bb1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
              "       0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
              "       1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
              "       1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.accuracy(y_test, ypreds)"
      ],
      "metadata": {
        "id": "QgG1xPUg1ULw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500d313d-752b-41e4-a88d-a6c87d7c51e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.98"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "  '''\n",
        "  The goal of this class is to create a LogisticRegression class, \n",
        "  that we will use as our model to classify data point into a corresponding class\n",
        "  '''\n",
        "  def __init__(self,lr,n_epochs):\n",
        "    self.lr = lr\n",
        "    self.n_epochs = n_epochs\n",
        "    self.train_losses = []\n",
        "    self.w = None\n",
        "    self.weight = []\n",
        "\n",
        "  def add_ones(self, x):\n",
        "    ##### WRITE YOUR CODE HERE #####\n",
        "    one = np.ones((len(x),1))\n",
        "    x = np.hstack((one,x))\n",
        "    return x\n",
        "    #### END CODE ####\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    ##### WRITE YOUR CODE HERE ####\n",
        "    sig = 1/(1+np.exp(-x@self.w))\n",
        "    return sig\n",
        "    #### END CODE ####\n",
        "\n",
        "  def cross_entropy(self, x, y_true):\n",
        "    ##### WRITE YOUR CODE HERE #####\n",
        "    y_pred = self.sigmoid(x)\n",
        "    loss = -(np.mean((y_true@np.log(y_pred))+(1-y_true)@np.log(1-y_pred)))\n",
        "    return loss\n",
        "    #### END CODE ####\n",
        "  \n",
        "  def predict_proba(self,x):  #This function will use the sigmoid function to compute the probalities\n",
        "    ##### WRITE YOUR CODE HERE #####\n",
        "    x= self.add_ones(x)\n",
        "    proba = self.sigmoid(x)\n",
        "    return proba\n",
        "    #### END CODE ####\n",
        "\n",
        "  def predict(self,x):\n",
        "    ##### WRITE YOUR CODE HERE #####\n",
        "    probas = self.predict_proba(x)\n",
        "    output = np.where(probas>=0.5,1,0)      #convert the probalities into 0 and 1 by using a treshold=0.5\n",
        "    return output\n",
        "    #### END CODE ####\n",
        "\n",
        "  def fit(self,x,y):\n",
        "    # Add ones to x\n",
        "    x = self.add_ones(x)\n",
        "    # reshape y if needed\n",
        "    #y = y_train.reshape(len(y_train),-1)\n",
        "    # Initialize w to zeros vector >>> (x.shape[1])\n",
        "    self.w = np.zeros((x.shape[1]))\n",
        "    for epoch in range(self.n_epochs):\n",
        "      # make predictions\n",
        "      ypred = self.sigmoid(x)\n",
        "\n",
        "      #compute the gradient\n",
        "      dl = (-x.T@(y-ypred))\n",
        "\n",
        "      #update rule\n",
        "      self.w =self.w - self.lr*dl\n",
        "      #Compute and append the training loss in a list\n",
        "      loss = self.cross_entropy(x,y)\n",
        "\n",
        "      self.weight.append(self.w)\n",
        "      self.train_losses.append(loss)\n",
        "\n",
        "      if epoch%100 == 0:\n",
        "        print(f'loss for epoch {epoch}  : {loss}')\n",
        "\n",
        "  def accuracy(self,y_true, y_pred):\n",
        "    ##### WRITE YOUR CODE HERE #####\n",
        "    acc = np.mean(y_true == y_pred)\n",
        "    return acc\n",
        "    #### END CODE ####"
      ],
      "metadata": {
        "id": "i5E0g7TDpIck"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(0.01,n_epochs=10000)\n",
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "VFZz_Y8myVJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971c7a4f-7efc-478f-b813-8eb380bf4731"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss for epoch 0  : 179.05196257302123\n",
            "loss for epoch 100  : 109.17589031411237\n",
            "loss for epoch 200  : 109.17409135711719\n",
            "loss for epoch 300  : 109.17408864556477\n",
            "loss for epoch 400  : 109.17408864138599\n",
            "loss for epoch 500  : 109.17408864137954\n",
            "loss for epoch 600  : 109.17408864137954\n",
            "loss for epoch 700  : 109.17408864137954\n",
            "loss for epoch 800  : 109.17408864137953\n",
            "loss for epoch 900  : 109.17408864137954\n",
            "loss for epoch 1000  : 109.17408864137954\n",
            "loss for epoch 1100  : 109.17408864137954\n",
            "loss for epoch 1200  : 109.17408864137954\n",
            "loss for epoch 1300  : 109.17408864137954\n",
            "loss for epoch 1400  : 109.17408864137954\n",
            "loss for epoch 1500  : 109.17408864137954\n",
            "loss for epoch 1600  : 109.17408864137954\n",
            "loss for epoch 1700  : 109.17408864137954\n",
            "loss for epoch 1800  : 109.17408864137954\n",
            "loss for epoch 1900  : 109.17408864137954\n",
            "loss for epoch 2000  : 109.17408864137954\n",
            "loss for epoch 2100  : 109.17408864137954\n",
            "loss for epoch 2200  : 109.17408864137954\n",
            "loss for epoch 2300  : 109.17408864137954\n",
            "loss for epoch 2400  : 109.17408864137954\n",
            "loss for epoch 2500  : 109.17408864137954\n",
            "loss for epoch 2600  : 109.17408864137954\n",
            "loss for epoch 2700  : 109.17408864137954\n",
            "loss for epoch 2800  : 109.17408864137954\n",
            "loss for epoch 2900  : 109.17408864137954\n",
            "loss for epoch 3000  : 109.17408864137954\n",
            "loss for epoch 3100  : 109.17408864137954\n",
            "loss for epoch 3200  : 109.17408864137954\n",
            "loss for epoch 3300  : 109.17408864137954\n",
            "loss for epoch 3400  : 109.17408864137954\n",
            "loss for epoch 3500  : 109.17408864137954\n",
            "loss for epoch 3600  : 109.17408864137954\n",
            "loss for epoch 3700  : 109.17408864137954\n",
            "loss for epoch 3800  : 109.17408864137954\n",
            "loss for epoch 3900  : 109.17408864137954\n",
            "loss for epoch 4000  : 109.17408864137954\n",
            "loss for epoch 4100  : 109.17408864137954\n",
            "loss for epoch 4200  : 109.17408864137954\n",
            "loss for epoch 4300  : 109.17408864137954\n",
            "loss for epoch 4400  : 109.17408864137954\n",
            "loss for epoch 4500  : 109.17408864137954\n",
            "loss for epoch 4600  : 109.17408864137954\n",
            "loss for epoch 4700  : 109.17408864137954\n",
            "loss for epoch 4800  : 109.17408864137954\n",
            "loss for epoch 4900  : 109.17408864137954\n",
            "loss for epoch 5000  : 109.17408864137954\n",
            "loss for epoch 5100  : 109.17408864137954\n",
            "loss for epoch 5200  : 109.17408864137954\n",
            "loss for epoch 5300  : 109.17408864137954\n",
            "loss for epoch 5400  : 109.17408864137954\n",
            "loss for epoch 5500  : 109.17408864137954\n",
            "loss for epoch 5600  : 109.17408864137954\n",
            "loss for epoch 5700  : 109.17408864137954\n",
            "loss for epoch 5800  : 109.17408864137954\n",
            "loss for epoch 5900  : 109.17408864137954\n",
            "loss for epoch 6000  : 109.17408864137954\n",
            "loss for epoch 6100  : 109.17408864137954\n",
            "loss for epoch 6200  : 109.17408864137954\n",
            "loss for epoch 6300  : 109.17408864137954\n",
            "loss for epoch 6400  : 109.17408864137954\n",
            "loss for epoch 6500  : 109.17408864137954\n",
            "loss for epoch 6600  : 109.17408864137954\n",
            "loss for epoch 6700  : 109.17408864137954\n",
            "loss for epoch 6800  : 109.17408864137954\n",
            "loss for epoch 6900  : 109.17408864137954\n",
            "loss for epoch 7000  : 109.17408864137954\n",
            "loss for epoch 7100  : 109.17408864137954\n",
            "loss for epoch 7200  : 109.17408864137954\n",
            "loss for epoch 7300  : 109.17408864137954\n",
            "loss for epoch 7400  : 109.17408864137954\n",
            "loss for epoch 7500  : 109.17408864137954\n",
            "loss for epoch 7600  : 109.17408864137954\n",
            "loss for epoch 7700  : 109.17408864137954\n",
            "loss for epoch 7800  : 109.17408864137954\n",
            "loss for epoch 7900  : 109.17408864137954\n",
            "loss for epoch 8000  : 109.17408864137954\n",
            "loss for epoch 8100  : 109.17408864137954\n",
            "loss for epoch 8200  : 109.17408864137954\n",
            "loss for epoch 8300  : 109.17408864137954\n",
            "loss for epoch 8400  : 109.17408864137954\n",
            "loss for epoch 8500  : 109.17408864137954\n",
            "loss for epoch 8600  : 109.17408864137954\n",
            "loss for epoch 8700  : 109.17408864137954\n",
            "loss for epoch 8800  : 109.17408864137954\n",
            "loss for epoch 8900  : 109.17408864137954\n",
            "loss for epoch 9000  : 109.17408864137954\n",
            "loss for epoch 9100  : 109.17408864137954\n",
            "loss for epoch 9200  : 109.17408864137954\n",
            "loss for epoch 9300  : 109.17408864137954\n",
            "loss for epoch 9400  : 109.17408864137954\n",
            "loss for epoch 9500  : 109.17408864137954\n",
            "loss for epoch 9600  : 109.17408864137954\n",
            "loss for epoch 9700  : 109.17408864137954\n",
            "loss for epoch 9800  : 109.17408864137954\n",
            "loss for epoch 9900  : 109.17408864137954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypred_train = model.predict(X_train)\n",
        "acc = model.accuracy(y_train,ypred_train)\n",
        "print(f\"The training accuracy is: {acc}\")\n",
        "print(\" \")\n",
        "\n",
        "ypred_test = model.predict(X_test)\n",
        "acc = model.accuracy(y_test,ypred_test)\n",
        "print(f\"The test accuracy is: {acc}\")"
      ],
      "metadata": {
        "id": "mAx8g6ZkyZLc",
        "outputId": "c471bf4d-4041-4a63-f7c7-867e3915d293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training accuracy is: 0.96\n",
            " \n",
            "The test accuracy is: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7L6yj05U5YNs"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}