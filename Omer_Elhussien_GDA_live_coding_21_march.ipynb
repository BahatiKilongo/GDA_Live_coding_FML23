{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerahmed12345elhussien/GDA_Live_coding_FML23/blob/main/Omer_Elhussien_GDA_live_coding_21_march.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GDA Implementation.\n",
        "\n",
        "Implement the Gaussian Discriminant Analysis (GDA) learning algorithm following the steps as discussed in class.\n",
        "\n",
        "INSTRUCTION: Rename your notebook as: <br>\n",
        "`firstName_LastName_Live_coding_GDA.ipynb`.\n",
        "\n",
        "Notes: \n",
        "* Do not use any built-in functions to complete a task;\n",
        "* Do not import additional libraries."
      ],
      "metadata": {
        "id": "g17Z46tmw2oZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aT5nlL-QTKwv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data\n",
        "def generate_data():\n",
        "  x, y = make_classification(n_samples= 1000, n_features=3, n_redundant=0, \n",
        "                           n_informative=3, random_state=1, \n",
        "                           n_clusters_per_class=1)\n",
        "  \n",
        "  return x,y\n",
        "\n",
        "x,y= generate_data() # get data\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-lL4Yq9Tbzn",
        "outputId": "152ad5e2-f6c1-41dc-a688-18e9f690723d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 3) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "aUOhIq9ghq1K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(x,y, train_size= 0.8):\n",
        "  \"\"\"\n",
        "  split_data(x,y, train_size= 0.8) accepts x, y, and train_size\n",
        "  It returns x,y splitted to train and test according to train_size\n",
        "  \"\"\"\n",
        "  # shuffle the data to randomize the train/test split\n",
        "  data_size=x.shape[0]\n",
        "  idx=np.random.permutation(data_size)\n",
        "  x=x[idx]\n",
        "  y=y[idx]\n",
        "  train_val=round(train_size*data_size)\n",
        "  x_train,y_train=x[0:train_val,:],y[0:train_val]\n",
        "  x_test,y_test=x[train_val:,:],y[train_val:]\n",
        "  return x_train,x_test,y_train,y_test\n"
      ],
      "metadata": {
        "id": "EUgiWLDhUAAK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_ones(x):\n",
        "  return np.insert(x,0,1,axis=1)"
      ],
      "metadata": {
        "id": "EX-sDPa4lxTa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test= split_data(x,y) # split your data into x_train, x_test, y_train, y_test\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr2Akm_A_FcJ",
        "outputId": "acfb2595-c385-4feb-9cb3-e99453642683"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800, 3) (800,) (200, 3) (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def covariance(x, mu):\n",
        "  # Easy way: cov= np.cov(x, rowvar=0) but do not use it. One can use it to assess his/her result.\n",
        "  m,d=x.shape\n",
        "  k=mu.shape[0]\n",
        "  resl_cov=np.zeros((k,d,d))\n",
        "  for cla_num in range(k):\n",
        "    for num_col1 in range(d):\n",
        "      for num_col2 in range(d):\n",
        "        resl_cov[cla_num][num_col1,num_col2]=(1/(m-1))*np.sum( (x[:,num_col1].reshape(-1,1)-mu[cla_num,num_col1])*(x[:,num_col2].reshape(-1,1)-mu[cla_num,num_col2])    )\n",
        "  return resl_cov"
      ],
      "metadata": {
        "id": "jpWE7Wem0jxC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean1=np.mean(x,axis=0);mean1=mean1.reshape(1,-1)"
      ],
      "metadata": {
        "id": "3rLf6sjr2qG2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.cov(x,rowvar=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoIesCSp0qgS",
        "outputId": "66c63d37-5b1e-44ad-fb0a-c6af98f778a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.84495325, 0.02790646, 1.00137533],\n",
              "       [0.02790646, 1.00170721, 0.05539176],\n",
              "       [1.00137533, 0.05539176, 1.74832   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "covariance(x,mean1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfvIQjEo7xCQ",
        "outputId": "6e002067-df7c-4a2c-9fec-00d763d78696"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1.84495325, 0.02790646, 1.00137533],\n",
              "        [0.02790646, 1.00170721, 0.05539176],\n",
              "        [1.00137533, 0.05539176, 1.74832   ]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multiv_gauss(x,mu,sigma):\n",
        "  x=x.reshape(1,-1)\n",
        "  d=mu.shape[0]\n",
        "  val_1=np.linalg.det(sigma)**0.5\n",
        "  val_2=(2*np.pi)**(d/2)\n",
        "  val_3=np.exp(-0.5*(x-mu)@np.linalg.inv(sigma)@(x-mu).T  )\n",
        "  z1=(1/(val_1*val_2) )*val_3\n",
        "  return z1.reshape(-1,1)"
      ],
      "metadata": {
        "id": "Qt_3y3TKD9wi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WHCAZE95_uST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GDA:\n",
        "  def __init__(self):\n",
        "    ## set mu, phi and sigma to None\n",
        "    self.mu= None\n",
        "    self.sigma= None\n",
        "    self.phi= None\n",
        "    \n",
        "  def fit(self,x,y):\n",
        "    k=np.unique(y).shape[0] # Number of class.\n",
        "    d=x.shape[1]  # input dim\n",
        "    m= x.shape[0] # Number of examples.\n",
        "    \n",
        "    ## Initialize mu, phi and sigma\n",
        "    self.mu= np.zeros((k,d))#: kxd, i.e., each row contains an individual class mu.\n",
        "    self.sigma= np.zeros((k,d,d))#: kxdxd, i.e., each row contains an individual class sigma.\n",
        "    self.phi= np.zeros((k,1))# k-dimension\n",
        "    #label_valu=np.unique(y)\n",
        "\n",
        "    ## START THE LEARNING: estimate mu, phi and sigma.\n",
        "    for class_num in range(k):\n",
        "      ##Calculating Phi\n",
        "      self.phi[class_num,0]=y_train[y==class_num].shape[0]/y.shape[0]\n",
        "      idx=np.where(y==class_num)\n",
        "      ##Caculating mu\n",
        "      self.mu[class_num]=np.mean(x[idx],axis=0)\n",
        "      ##Caculating Sigma\n",
        "      self.sigma[class_num]=covariance(x, self.mu[class_num].reshape(1,-1))\n",
        "    \n",
        "    \n",
        "  def predict_proba(self,x):\n",
        "    # reshape or flatt x.\n",
        "    n=x.shape[0]\n",
        "    d= self.mu.shape[1]\n",
        "    k_class=self.mu.shape[0] # Number of classes we have in our case it's k = 2\n",
        "    det_sigma=0\n",
        "    y_pred=np.zeros((n,k_class))\n",
        "    for i in range(k_class):\n",
        "      det_sigma=np.linalg.det(self.sigma[i])**0.5\n",
        "      inv_sigma=np.linalg.inv(self.sigma[i])\n",
        "      for j in range(n):\n",
        "        val_2=(2*np.pi)**(d/2)\n",
        "        val_3=np.exp(-0.5*(x[j].reshape(1,-1)-self.mu[i].reshape(1,-1))@inv_sigma@(x[j].reshape(1,-1)-self.mu[i]).reshape(1,-1).T  )\n",
        "        z1=(1/(det_sigma*val_2) )*val_3\n",
        "        y_pred[j,i]=z1*self.phi[i]\n",
        "    return y_pred\n",
        "\n",
        "    ## START THE LEARNING: estimate mu, phi and sigma.\n",
        "\n",
        "  def predict(self,x):\n",
        "    y_pred=self.predict_proba(x)\n",
        "    y_res=np.argmax(y_pred,axis=1)\n",
        "    return y_res\n",
        "  \n",
        "  def accuracy(self, y, ypreds):\n",
        "    acc=np.mean(y==ypreds)*100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "1ocKLDAfceF0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= GDA()\n",
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "l_qO0Yp1c3Is"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.mu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5w5DD1pIlB_",
        "outputId": "dff39815-926b-4988-f0b7-c22fda9e2286"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.02275133,  1.04572584,  1.00413266],\n",
              "       [-0.98392793,  0.97138702, -0.92252973]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.phi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNmZKncCIoHf",
        "outputId": "d84363b1-19d9-4227-f2bf-51a707a844fd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.51],\n",
              "       [0.49]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.sigma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz3z8VEPIqov",
        "outputId": "cfb99ca2-a81b-4b3c-e6a8-fe737a917c1e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[2.78583678, 0.03864649, 1.92964772],\n",
              "        [0.03864649, 0.98935083, 0.07950682],\n",
              "        [1.92964772, 0.07950682, 2.62243381]],\n",
              "\n",
              "       [[2.86647281, 0.04163371, 2.00706837],\n",
              "        [0.04163371, 0.98946149, 0.08237492],\n",
              "        [2.00706837, 0.08237492, 2.69676728]]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yproba= model.predict_proba(X_test)\n",
        "yproba"
      ],
      "metadata": {
        "id": "NKY1eojY1l4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3183010-c874-4593-ca67-a2f261fc8109"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.49750663e-03, 1.20875369e-02],\n",
              "       [9.26930267e-04, 3.11908708e-03],\n",
              "       [9.30518423e-03, 2.64038083e-03],\n",
              "       [1.71393978e-03, 7.39199929e-03],\n",
              "       [1.30131722e-03, 8.77613859e-03],\n",
              "       [4.95218359e-03, 1.45493142e-02],\n",
              "       [1.23035834e-02, 6.59724126e-03],\n",
              "       [7.22695457e-03, 1.16909529e-02],\n",
              "       [1.27122141e-02, 5.04758484e-03],\n",
              "       [2.31717523e-03, 3.15927745e-03],\n",
              "       [1.13657415e-03, 6.23686102e-03],\n",
              "       [1.06674704e-02, 1.10726942e-02],\n",
              "       [4.28448864e-03, 1.15375044e-02],\n",
              "       [7.65597737e-04, 2.19145615e-03],\n",
              "       [3.69590478e-04, 1.70507796e-04],\n",
              "       [1.37823207e-03, 3.03719737e-03],\n",
              "       [6.68637037e-05, 6.00993700e-04],\n",
              "       [4.57469926e-03, 1.37957920e-02],\n",
              "       [8.46249380e-04, 1.76366916e-03],\n",
              "       [3.97367514e-03, 1.38652238e-02],\n",
              "       [8.08404126e-04, 2.38170181e-03],\n",
              "       [1.27549862e-02, 4.71680456e-03],\n",
              "       [3.39172279e-03, 8.27430000e-03],\n",
              "       [9.35720105e-04, 3.45558281e-03],\n",
              "       [1.11547726e-02, 5.57220975e-03],\n",
              "       [3.40900796e-03, 8.67712849e-03],\n",
              "       [9.05600125e-03, 1.49643709e-02],\n",
              "       [9.47331818e-03, 3.87236822e-03],\n",
              "       [1.26722456e-02, 1.36677101e-02],\n",
              "       [7.03356277e-03, 4.87224965e-03],\n",
              "       [1.08251785e-02, 5.55912763e-03],\n",
              "       [1.94240590e-04, 2.77651504e-03],\n",
              "       [9.83329341e-03, 1.58088925e-02],\n",
              "       [5.32616528e-03, 1.37293865e-02],\n",
              "       [1.53721789e-03, 1.24871290e-03],\n",
              "       [1.11055863e-03, 1.08041222e-03],\n",
              "       [2.09818040e-06, 4.02100886e-06],\n",
              "       [4.57231293e-03, 1.26815590e-02],\n",
              "       [7.47386381e-03, 1.18931816e-02],\n",
              "       [2.43942885e-03, 5.36276682e-03],\n",
              "       [2.42472742e-03, 8.72403323e-03],\n",
              "       [7.08722534e-03, 3.29766181e-03],\n",
              "       [1.30318645e-02, 8.08212514e-03],\n",
              "       [5.06427943e-04, 1.27711128e-03],\n",
              "       [6.43294745e-04, 4.43417068e-04],\n",
              "       [1.54226623e-04, 1.37051481e-04],\n",
              "       [8.74701486e-03, 3.89087331e-03],\n",
              "       [2.84413831e-03, 9.91953879e-04],\n",
              "       [7.77457630e-04, 1.99858915e-03],\n",
              "       [1.35363701e-02, 5.98135038e-03],\n",
              "       [8.05565018e-03, 4.86938506e-03],\n",
              "       [1.39717614e-02, 5.31069187e-03],\n",
              "       [1.84796980e-03, 6.64549375e-03],\n",
              "       [4.55625311e-03, 3.27277225e-03],\n",
              "       [1.01187379e-03, 2.16636105e-03],\n",
              "       [1.63022764e-02, 6.28737042e-03],\n",
              "       [5.93331530e-03, 4.21230509e-03],\n",
              "       [5.73859382e-03, 7.30632562e-03],\n",
              "       [9.80015490e-03, 3.63916506e-03],\n",
              "       [1.37973495e-02, 6.65478501e-03],\n",
              "       [7.37598208e-03, 1.20336716e-02],\n",
              "       [1.36067924e-02, 8.38382025e-03],\n",
              "       [3.62063990e-03, 7.75428514e-04],\n",
              "       [2.74567175e-03, 1.37514729e-03],\n",
              "       [5.29521743e-03, 1.09945589e-02],\n",
              "       [5.31718208e-03, 2.04351621e-03],\n",
              "       [8.16021953e-03, 1.29143401e-02],\n",
              "       [5.63179967e-03, 1.30707734e-02],\n",
              "       [5.48315344e-03, 2.48564352e-03],\n",
              "       [1.19575992e-03, 2.44961213e-04],\n",
              "       [1.32796791e-02, 4.71302403e-03],\n",
              "       [1.65220198e-02, 7.07425979e-03],\n",
              "       [9.08035968e-03, 5.23477232e-03],\n",
              "       [5.36192408e-03, 9.25798735e-03],\n",
              "       [4.98565540e-03, 1.19654086e-02],\n",
              "       [2.42802215e-03, 1.05631002e-02],\n",
              "       [1.55579834e-02, 6.26316556e-03],\n",
              "       [7.98283931e-03, 2.91179392e-03],\n",
              "       [2.76169159e-03, 1.24014350e-02],\n",
              "       [1.47053901e-02, 7.45618902e-03],\n",
              "       [6.64567737e-03, 8.55768530e-03],\n",
              "       [2.43005420e-04, 9.18635860e-04],\n",
              "       [4.35213395e-03, 3.80845077e-03],\n",
              "       [7.27990543e-04, 3.96296300e-04],\n",
              "       [1.01428595e-02, 1.46443758e-02],\n",
              "       [2.34368996e-03, 6.99323992e-03],\n",
              "       [3.75671938e-03, 1.45177731e-02],\n",
              "       [4.43837061e-03, 1.15052402e-02],\n",
              "       [9.02901122e-03, 7.21202036e-03],\n",
              "       [4.54913862e-03, 2.35278353e-03],\n",
              "       [1.32068177e-04, 9.39478579e-04],\n",
              "       [4.79245379e-04, 5.80916830e-04],\n",
              "       [4.59654077e-03, 1.52019838e-02],\n",
              "       [7.73861313e-05, 5.40002419e-05],\n",
              "       [3.40477696e-04, 4.89055082e-05],\n",
              "       [6.48285096e-03, 3.42888072e-03],\n",
              "       [1.09840752e-02, 5.38162597e-03],\n",
              "       [2.20758299e-03, 9.02922946e-04],\n",
              "       [3.82536113e-03, 1.36126425e-02],\n",
              "       [1.22317203e-03, 6.52352429e-04],\n",
              "       [6.07734080e-03, 1.41186659e-02],\n",
              "       [1.55345971e-03, 1.04899568e-03],\n",
              "       [3.65277484e-03, 1.39715352e-02],\n",
              "       [5.27901097e-03, 2.56813444e-03],\n",
              "       [3.04191585e-03, 1.13683684e-02],\n",
              "       [3.43556118e-03, 1.92333280e-03],\n",
              "       [6.98641859e-03, 6.99152504e-03],\n",
              "       [1.05025599e-02, 4.39720121e-03],\n",
              "       [1.34558735e-02, 4.40641645e-03],\n",
              "       [3.77482921e-04, 9.18308796e-04],\n",
              "       [1.63852087e-03, 1.00634768e-02],\n",
              "       [7.79176007e-03, 8.22274790e-03],\n",
              "       [2.52290515e-03, 1.04254209e-02],\n",
              "       [4.31125427e-03, 1.34199832e-02],\n",
              "       [1.12714519e-02, 1.41141279e-02],\n",
              "       [1.57408927e-02, 6.74503202e-03],\n",
              "       [3.73424121e-03, 7.02002376e-03],\n",
              "       [1.66067040e-02, 5.76328991e-03],\n",
              "       [9.31921232e-03, 2.93010663e-03],\n",
              "       [7.92950590e-03, 9.58325012e-03],\n",
              "       [1.49512979e-02, 8.30220134e-03],\n",
              "       [4.98472966e-03, 1.20261564e-03],\n",
              "       [6.75159469e-04, 2.81897770e-04],\n",
              "       [2.96625500e-04, 2.34420569e-03],\n",
              "       [1.18164928e-02, 1.03460524e-02],\n",
              "       [4.20176016e-04, 8.18342553e-04],\n",
              "       [1.26034163e-02, 6.27380490e-03],\n",
              "       [6.02278288e-05, 5.52494994e-04],\n",
              "       [3.02714770e-04, 1.37177196e-04],\n",
              "       [6.08103833e-03, 1.51455385e-02],\n",
              "       [5.20464895e-03, 8.07251840e-03],\n",
              "       [2.17552573e-03, 2.57620458e-03],\n",
              "       [3.96069976e-03, 3.05232155e-03],\n",
              "       [1.50946768e-03, 5.17437363e-04],\n",
              "       [6.34467210e-03, 1.71007444e-03],\n",
              "       [6.79986618e-03, 1.36612165e-02],\n",
              "       [3.89089481e-04, 2.69812439e-03],\n",
              "       [1.01353267e-02, 2.83128602e-03],\n",
              "       [6.60340442e-03, 3.59968520e-03],\n",
              "       [1.35600157e-03, 7.34429450e-04],\n",
              "       [5.90527693e-03, 2.65946575e-03],\n",
              "       [1.02146159e-02, 1.26393922e-02],\n",
              "       [5.33653659e-03, 4.19813725e-03],\n",
              "       [1.95174987e-03, 1.77866792e-03],\n",
              "       [5.38856504e-04, 3.13238468e-03],\n",
              "       [9.45097814e-03, 8.01964704e-03],\n",
              "       [5.23421003e-03, 1.01787295e-02],\n",
              "       [1.13135797e-02, 9.71777986e-03],\n",
              "       [5.20936044e-03, 5.44008120e-03],\n",
              "       [2.93655597e-03, 3.89080174e-03],\n",
              "       [6.73235857e-03, 2.14489234e-03],\n",
              "       [7.24427815e-03, 6.89837851e-03],\n",
              "       [2.71855003e-03, 1.09253394e-02],\n",
              "       [2.37383345e-03, 6.09958711e-03],\n",
              "       [1.52873196e-02, 6.06968899e-03],\n",
              "       [4.81477354e-04, 4.02158843e-03],\n",
              "       [4.07574199e-03, 6.41964201e-03],\n",
              "       [1.00149081e-02, 1.15616864e-02],\n",
              "       [5.21622742e-03, 2.29328281e-03],\n",
              "       [1.62290621e-02, 5.87943329e-03],\n",
              "       [1.51574221e-02, 7.84659188e-03],\n",
              "       [3.04273226e-04, 6.98012915e-05],\n",
              "       [4.85342765e-03, 1.22405843e-02],\n",
              "       [1.11306798e-02, 6.00356510e-03],\n",
              "       [8.58425052e-03, 3.66727028e-03],\n",
              "       [6.63046282e-03, 1.85422625e-03],\n",
              "       [5.44267428e-03, 3.46700117e-03],\n",
              "       [1.46895786e-02, 7.88905760e-03],\n",
              "       [8.11581429e-03, 3.00072457e-03],\n",
              "       [1.64709103e-03, 5.51583748e-03],\n",
              "       [6.26419082e-03, 4.03717183e-03],\n",
              "       [3.27799410e-03, 9.43374366e-03],\n",
              "       [3.13868909e-03, 8.37178682e-04],\n",
              "       [3.19270382e-03, 9.29547524e-04],\n",
              "       [4.32617300e-04, 2.65624486e-04],\n",
              "       [4.61256213e-03, 1.34044968e-02],\n",
              "       [1.49726796e-03, 8.50396911e-03],\n",
              "       [6.23625037e-03, 3.86593474e-03],\n",
              "       [1.09961124e-02, 1.16801605e-02],\n",
              "       [2.68565949e-03, 9.24990881e-03],\n",
              "       [1.16737303e-03, 4.16826622e-03],\n",
              "       [8.59694673e-07, 4.86565383e-07],\n",
              "       [6.78056419e-03, 3.88129328e-03],\n",
              "       [5.74258015e-03, 1.52777893e-02],\n",
              "       [9.21225841e-03, 4.57384911e-03],\n",
              "       [3.69939907e-03, 1.00709100e-02],\n",
              "       [8.54950203e-05, 5.11033766e-04],\n",
              "       [2.91714720e-03, 8.36104352e-03],\n",
              "       [1.22663073e-02, 4.62891727e-03],\n",
              "       [6.15880898e-03, 1.35082479e-03],\n",
              "       [4.54316010e-03, 1.97007561e-03],\n",
              "       [2.59240237e-03, 8.73526769e-03],\n",
              "       [5.86919360e-03, 1.70129765e-03],\n",
              "       [3.51583922e-03, 4.88609128e-03],\n",
              "       [6.77310629e-04, 2.11308483e-03],\n",
              "       [5.09283678e-03, 2.65918697e-03],\n",
              "       [1.31806264e-03, 7.61629756e-03],\n",
              "       [4.23473251e-03, 9.16259896e-03],\n",
              "       [1.43562492e-03, 8.16154245e-03],\n",
              "       [8.81333458e-05, 2.41492120e-05]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k8JdS3Q9khdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypreds= model.predict(X_test)\n",
        "ypreds\n"
      ],
      "metadata": {
        "id": "D4clV6PK1UJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b1b248-8af2-4d19-e768-ab0ee89f9863"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "       0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
              "       1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
              "       1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_GDA=model.accuracy(y_test, ypreds)\n",
        "\n",
        "print(acc_GDA)"
      ],
      "metadata": {
        "id": "QgG1xPUg1ULw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72db13a-6b3c-4842-e9d6-b8a9bc0f96ff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpXYY-yj1UOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Logistic_Regression:\n",
        "  # Algorithm has to be replaced, e.g. Linear_regression.\n",
        "  #######The class Constructor##########\n",
        "  def __init__(self,arg,lr,n_epoc):\n",
        "    self.arg= arg\n",
        "    self.theta=None\n",
        "    self.lr=lr #10e-5#0.01\n",
        "    self.n_epoc=n_epoc\n",
        "    self.training_loss=[]\n",
        "\n",
        "  #######The class Constructor##########\n",
        "  def intialize_w(self,x):\n",
        "    return np.zeros((x.shape[1],1))\n",
        "  \n",
        "                  ##############Class methods#################\n",
        "  #######Set the Batch size##########\n",
        "  def set_BS(self,x):\n",
        "    appr_size=int(np.ceil(np.log2(x)))\n",
        "    self.batch_size=2**appr_size\n",
        "\n",
        "  #######Set Beta for momentum##########\n",
        "  def set_Beta(self,x=0.99):\n",
        "    self.beta=x\n",
        "\n",
        "  #######Set Eps for termination##########\n",
        "  def set_Eps(self,eps1=10e-8):\n",
        "    self.eps=eps1\n",
        "\n",
        "  #######Sigmoid fun.##########\n",
        "  def sigmoid(self,x):\n",
        "    z=x@self.theta\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "  #######Cross_entropy Loss fun.##########\n",
        "  def cross_entropy(self,y_true,y_pred):\n",
        "    loss= -np.mean(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))\n",
        "    return loss\n",
        "\n",
        "  #######Gradient of the Loss fun.##########\n",
        "  def grad_cross_entropy(self,x,y,y_pred):\n",
        "    return (-1/x.shape[0])*x.T@(y-y_pred )\n",
        "  \n",
        "  #######Batch Gradient Descent##########\n",
        "  def BGD(self,x,y,mom=False): #Batch Gradient Descent\n",
        "    self.theta=self.intialize_w(x)\n",
        "    v_0=0\n",
        "    for num_epoc in range(self.n_epoc):\n",
        "      # Make predictions\n",
        "      y_pred=self.sigmoid(x)\n",
        "      # compute the loss\n",
        "      loss= self.cross_entropy(y,y_pred)\n",
        "      # compute the gradient\n",
        "      dl= self.grad_cross_entropy(x,y,y_pred)\n",
        "      # Update rules\n",
        "      if mom==False:\n",
        "        old_theta=self.theta\n",
        "        self.theta= self.theta-self.lr*dl\n",
        "        if np.linalg.norm(old_theta-self.theta)<self.eps:\n",
        "          break\n",
        "      else:\n",
        "        old_theta=self.theta\n",
        "        v_0=self.beta*v_0+(1-self.beta)*dl\n",
        "        self.theta=self.theta-self.lr*v_0\n",
        "        if np.linalg.norm(old_theta-self.theta)<self.eps:\n",
        "          break\n",
        "        \n",
        "      ## APPEND loss\n",
        "      self.training_loss.append(loss)\n",
        "      if num_epoc%1000==0:\n",
        "        print(f\"N_epoc is {num_epoc}, with loss {loss}\")\n",
        "    return self.training_loss\n",
        "\n",
        "  #######Stochastic Gradient Descent##########\n",
        "  def SGD(self,x,y,mom=False):\n",
        "    self.theta=self.intialize_w(x)\n",
        "    v_0=0\n",
        "    mean_loss=[]\n",
        "    #break_value=0\n",
        "    for num_epoc in range(self.n_epoc):\n",
        "      idx=np.random.permutation(x.shape[0])\n",
        "      for j in range(x.shape[0]):\n",
        "        x_valu=x[idx[j],:].reshape(1,-1)\n",
        "        y_valu=y[idx[j],:].reshape(-1,1)\n",
        "        # Make predictions\n",
        "        y_pred=self.sigmoid(x_valu)\n",
        "        # compute the loss\n",
        "        loss=self.cross_entropy(y_valu,y_pred)\n",
        "        # compute the gradient\n",
        "        dl=self.grad_cross_entropy(x_valu,y_valu,y_pred)\n",
        "        # Update rules\n",
        "        if mom==False:\n",
        "          old_theta=self.theta\n",
        "          self.theta= self.theta-self.lr*dl\n",
        "          if np.linalg.norm(old_theta-self.theta)<self.eps:\n",
        "            break_value=1\n",
        "            break\n",
        "        else:\n",
        "          old_theta=self.theta\n",
        "          v_0=self.beta*v_0+(1-self.beta)*dl\n",
        "          self.theta=self.theta-self.lr*v_0\n",
        "          if np.linalg.norm(old_theta-self.theta)<self.eps:\n",
        "            break_value=1\n",
        "            break\n",
        "\n",
        "        ## APPEND loss\n",
        "        mean_loss.append(loss)\n",
        "      \n",
        "      if num_epoc%1000==0:\n",
        "        print(f\"N_epoc is {num_epoc}, with loss {loss}\")\n",
        "      self.training_loss.append(np.mean(mean_loss))\n",
        "      #if break_value==1:\n",
        "      # break\n",
        "\n",
        "    return self.training_loss\n",
        "\n",
        "  #######Mini-Batch Gradient Descent##########\n",
        "  def MBGD(self,x,y,mom=False):\n",
        "    self.theta=self.intialize_w(x)\n",
        "    mean_loss=[]\n",
        "    v_0=0\n",
        "    #break_value=0\n",
        "    for num_epoc in range(self.n_epoc):\n",
        "      count_0= 0\n",
        "      mean_loss=[]\n",
        "      idx=np.random.permutation(x.shape[0])\n",
        "      x=x[idx]\n",
        "      y=y[idx]\n",
        "      while count_0<x.shape[0]:\n",
        "        count_1=count_0+self.batch_size\n",
        "        if count_1>x.shape[0]:\n",
        "          count_1=x.shape[0]\n",
        "        x_valu=x[count_0:count_1,:]\n",
        "        y_valu=y[count_0:count_1].reshape(-1,1)\n",
        "        # Make predictions\n",
        "        y_pred=self.sigmoid(x_valu)\n",
        "        # compute the loss\n",
        "        loss=self.cross_entropy(y_valu,y_pred)\n",
        "        # compute the gradient\n",
        "        dl=self.grad_cross_entropy(x_valu,y_valu,y_pred)\n",
        "        # Update rules\n",
        "        if mom==False:\n",
        "          old_theta=self.theta\n",
        "          self.theta=self.theta-self.lr*dl\n",
        "          if np.linalg.norm(old_theta-self.theta)<self.eps:\n",
        "            break_value=1\n",
        "            break\n",
        "        else:\n",
        "          old_theta=self.theta\n",
        "          v_0=self.beta*v_0+(1-self.beta)*dl\n",
        "          self.theta=self.theta-self.lr*v_0\n",
        "          if np.linalg.norm(old_theta-self.theta)<self.eps:\n",
        "            break_value=1\n",
        "            break\n",
        "      \n",
        "        mean_loss.append(loss)\n",
        "        count_0=count_1\n",
        "      if num_epoc%1000==0:\n",
        "        print(f\"N_epoc is {num_epoc}, with loss {loss}\")\n",
        "\n",
        "      self.training_loss.append(np.mean(mean_loss))\n",
        "      #if break_value==1:\n",
        "       # break\n",
        "\n",
        "    return self.training_loss\n",
        "\n",
        "  #######Fit the model parameters##########\n",
        "  def fit(self,x,y):\n",
        "    x=add_ones(x)\n",
        "    y=y.reshape(-1,1)\n",
        "    #\n",
        "    if self.arg==1: #For Gradient descent\n",
        "      return self.BGD(x,y)\n",
        "    #\n",
        "    elif self.arg==2: #For Stochastic Gradient descent\n",
        "      return self.SGD(x,y)\n",
        "    #\n",
        "    elif self.arg==3: #For MBGD\n",
        "      return self.MBGD(x,y)\n",
        "    #\n",
        "    elif self.arg==4: #For GD with Momentum\n",
        "      return self.BGD(x,y,mom=True)\n",
        "    #\n",
        "    elif self.arg==5: #For SGD with Momentum\n",
        "      return self.SGD(x,y,mom=True)\n",
        "    #\n",
        "    elif self.arg==6: #For MBGD with Momentum\n",
        "      return self.MBGD(x,y,mom=True)\n",
        "    \n",
        "  #######Make Prediction##########\n",
        "  def predict(self,x):\n",
        "    x=add_ones(x)\n",
        "    y_pred=self.sigmoid(x)\n",
        "    y_resl=[1 if p>0.5 else 0 for p in y_pred]\n",
        "    return y_resl\n",
        "  \n",
        "  def accuracy_fun(self,y,y_pred):\n",
        "    acc_value=np.mean(y==y_pred)*100\n",
        "    return acc_value\n",
        "    "
      ],
      "metadata": {
        "id": "8cvRcUO2rtKo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Mini-Batch Gradient Descent with Momentum"
      ],
      "metadata": {
        "id": "eP6lglpek9k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For MBGD with Momentum\n",
        "#X_train, X_test, y_train, y_test\n",
        "modl5=Logistic_Regression(arg=6,lr=0.01,n_epoc=10000) \n",
        "modl5.set_Eps()\n",
        "modl5.set_Beta()\n",
        "modl5.set_BS(8)\n",
        "loss_5=modl5.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "372Zr-6Xk4DN",
        "outputId": "0320bd15-600b-4db8-b263-d7acf0ca9aaa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N_epoc is 0, with loss 0.5501623107831122\n",
            "N_epoc is 1000, with loss 0.11946400995581505\n",
            "N_epoc is 2000, with loss 0.0748962724808178\n",
            "N_epoc is 3000, with loss 0.018902429030721382\n",
            "N_epoc is 4000, with loss 0.09245317404038121\n",
            "N_epoc is 5000, with loss 0.17325754254346562\n",
            "N_epoc is 6000, with loss 0.016695603034836956\n",
            "N_epoc is 7000, with loss 0.020406044993600915\n",
            "N_epoc is 8000, with loss 0.026922514026016992\n",
            "N_epoc is 9000, with loss 0.019299474756518116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_5)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "eMstACF_lImS",
        "outputId": "e3cd13a1-8e7a-4d44-d046-9dbc3524485c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train=modl5.predict(X_train)\n",
        "acc1=modl5.accuracy_fun(y_train,y_pred_train)\n",
        "print(f\"The prediction accuracy is: {acc1}\")\n",
        "print(\"\")\n",
        "y_pred_test=modl5.predict(X_test)\n",
        "acc2_logistic=modl5.accuracy_fun(y_test,y_pred_test)\n",
        "print(f\"The prediction accuracy is: {acc2_logistic}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdQHDRYClJCd",
        "outputId": "55010f92-8e22-4621-8e60-6b275abf2896"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction accuracy is: 96.0\n",
            "\n",
            "The prediction accuracy is: 95.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The prediction accuracy for Logistic Regression is: {acc2_logistic}\")\n",
        "print(f\"The prediction accuracy for  GDA is: {acc_GDA}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFvzI6OKmKEG",
        "outputId": "e08765e7-65d0-4836-f532-d9d27460f9bf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction accuracy for Logistic Regression is: 95.0\n",
            "The prediction accuracy for  GDA is: 94.5\n"
          ]
        }
      ]
    }
  ]
}